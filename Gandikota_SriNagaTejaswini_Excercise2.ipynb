{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejaswini-151999/SriNagTejaswiniGandikota_INFO5731_Fall2024/blob/main/Gandikota_SriNagaTejaswini_Excercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your answer here\n",
        "#How does working at home compare to working in an office in terms of employee productivity and well-being?\n",
        "#DATA COLLECTION\n",
        "\n",
        "#What to Gather:\n",
        "\n",
        "#Productivity: Track through company provided tools the numbers of jobs completed and hours worked.\n",
        "#Well-being: Assess perceived stress and general happiness through self-report questionnaires.\n",
        "#Feedback: Take employees' opinions by implementing surveys and interviews.\n",
        "\n",
        "#How Much Data:\n",
        "#Quantitative: Sample size of approximately 200 employees over a period of 6-12 months.\n",
        "\n",
        "#Qualitative: Administer survey to 50-100 employees; interview 15 to 20 for further clarification.\n",
        "\n",
        "#Steps:\n",
        "\n",
        "#1. Define Goals : Define what you want to find out.\n",
        "#2. Create Tools: Prepare questionnaires and interviewing tools.\n",
        "#3. Research Participants: Invite employees to participate in the research and obtain their consent.\n",
        "#4. Data Collection : Gather productivity data; conduct some survey and interviews.\n",
        "#5. Store Data: Store everything online in a secured manner with data organized.\n",
        "#6. Analyze: Analyzing tools review numbers and find trends in feedback. Report and sum results, suggesting improvements if needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoS1pYLKFpaY",
        "outputId": "87a304d1-d308-4296-b7ac-5b8cc514901d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/customer_data.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "CqCH9573GjBf",
        "outputId": "7211f7f4-bb7d-426b-bb55-f20e47c96408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ec3b4914-7f25-4b6c-b0c1-e627cfde0cd0\", \"customer_data.csv\", 1)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Define constants for testing\n",
        "API_URL = \"https://jsonplaceholder.typicode.com/posts\"\n",
        "NUM_SAMPLES = 1000\n",
        "OUTPUT_FILE = \"/content/test_data.csv\"\n",
        "\n",
        "def fetch_data(api_url, num_samples):\n",
        "    \"\"\"Fetch data from the API and return a list of records.\"\"\"\n",
        "    data = []\n",
        "    for i in range(num_samples):\n",
        "        try:\n",
        "            response = requests.get(api_url)\n",
        "            response.raise_for_status()\n",
        "            data.append(response.json())\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Request failed for sample {i+1}: {e}\")\n",
        "    return data\n",
        "\n",
        "def save_data_to_csv(data, file_name):\n",
        "    \"\"\"Save the data to a CSV file.\"\"\"\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(file_name, index=False)\n",
        "    print(f\"Data saved to {file_name}\")\n",
        "\n",
        "# Fetch and save data\n",
        "print(f\"Starting to fetch {NUM_SAMPLES} samples...\")\n",
        "data = fetch_data(API_URL, NUM_SAMPLES)\n",
        "print(\"Saving data...\")\n",
        "save_data_to_csv(data, OUTPUT_FILE)\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "514e70a0-2d1a-412e-d086-ad6412b586ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to fetch 1000 samples...\n",
            "Saving data...\n",
            "Data saved to /content/test_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QozAvhg_KtVH",
        "outputId": "df57d6a2-61cb-4266-820b-77902c6fbd50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaGLbSHHB8Ej"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def fetch_articles(keyword, year_range, num_articles=1000):\n",
        "    base_url = \"https://api.semanticscholar.org/v1/paper/search\"\n",
        "    articles = []\n",
        "\n",
        "    for start in range(0, num_articles, 100):\n",
        "        params = {\n",
        "            'query': keyword,\n",
        "            'year': year_range,\n",
        "            'offset': start,\n",
        "            'limit': 100\n",
        "        }\n",
        "        response = requests.get(base_url, params=params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            articles.extend(data.get('data', []))\n",
        "            if len(data.get('data', [])) < 100:\n",
        "                break\n",
        "        else:\n",
        "            print(\"Error fetching data:\", response.status_code)\n",
        "            break\n",
        "\n",
        "    return articles\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = \"XYZ\"\n",
        "year_range = \"2014-2024\"\n",
        "articles = fetch_articles(keyword, year_range)\n",
        "\n",
        "print(f\"Total articles fetched: {len(articles)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdj11eM4PMCH",
        "outputId": "ae3c26d5-5efc-44ca-9e6d-939713540b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 404\n",
            "Total articles fetched: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_article_info(articles):\n",
        "    article_info = []\n",
        "    for article in articles:\n",
        "        info = {\n",
        "            \"title\": article.get(\"title\", \"N/A\"),\n",
        "            \"venue\": article.get(\"venue\", \"N/A\"),\n",
        "            \"year\": article.get(\"year\", \"N/A\"),\n",
        "            \"authors\": [author['name'] for author in article.get(\"authors\", [])],\n",
        "            \"abstract\": article.get(\"abstract\", \"N/A\")\n",
        "        }\n",
        "        article_info.append(info)\n",
        "    return article_info\n",
        "\n",
        "article_details = extract_article_info(articles)\n"
      ],
      "metadata": {
        "id": "S_Iw7pyUPQoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(article_details)\n",
        "df.to_csv(\"articles.csv\", index=False)\n",
        "print(\"Articles saved to articles.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ3XKGAmPVS2",
        "outputId": "000bd41e-c5cc-4ab7-8d2c-54e5eef70517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles saved to articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25d44b1-80be-4cae-ba3a-dcb38632edfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (3.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.27.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from tweepy) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tweepy pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "\n",
        "# Replace with your actual credentials\n",
        "API_KEY = 'YOUR_API_KEY'\n",
        "API_SECRET_KEY = 'YOUR_API_SECRET_KEY'\n",
        "ACCESS_TOKEN = 'YOUR_ACCESS_TOKEN'\n",
        "ACCESS_TOKEN_SECRET = 'YOUR_ACCESS_TOKEN_SECRET'\n",
        "\n",
        "# Authenticate to Twitter\n",
        "auth = tweepy.OAuth1UserHandler(API_KEY, API_SECRET_KEY)\n",
        "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Verify credentials\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"Authentication OK\")\n",
        "except Exception as e:\n",
        "    print(\"Error during authentication:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5zfR2p2THQk",
        "outputId": "8fb2f1fc-bedb-4a7e-ebcd-8e9cc5a1666d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during authentication: 401 Unauthorized\n",
            "89 - Invalid or expired token.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_tweets(query, count=100):\n",
        "    tweets = []  # Initialize the list to hold tweets\n",
        "\n",
        "    try:\n",
        "        for tweet in tweepy.Cursor(api.search_tweets, q=query, lang=\"en\", tweet_mode='extended').items(count):\n",
        "            tweet_data = {\n",
        "                'username': tweet.user.screen_name,\n",
        "                'tweet_id': tweet.id_str,\n",
        "                'text': tweet.full_text,\n",
        "                'created_at': tweet.created_at,\n",
        "                'retweet_count': tweet.retweet_count,\n",
        "                'like_count': tweet.favorite_count\n",
        "            }\n",
        "            tweets.append(tweet_data)\n",
        "    except Exception as e:\n",
        "        print(\"Error fetching tweets:\", e)\n",
        "\n",
        "    return tweets\n"
      ],
      "metadata": {
        "id": "vp_izADhTn0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"#YourHashtag\"  # Change to your desired hashtag or keyword\n",
        "\n",
        "# Fetch tweets\n",
        "tweets = fetch_tweets(query, count=100)  # Fetch 100 tweets\n",
        "print(f\"Total tweets collected: {len(tweets)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6_CVWFOTr9q",
        "outputId": "c743c1af-26d3-4937-dc82-209703d8e872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching tweets: 401 Unauthorized\n",
            "89 - Invalid or expired token.\n",
            "Total tweets collected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a DataFrame only if tweets were collected\n",
        "if len(tweets) > 0:\n",
        "    try:\n",
        "        df = pd.DataFrame(tweets)\n",
        "        print(\"DataFrame created successfully!\")\n",
        "        print(df.head())\n",
        "    except Exception as e:\n",
        "        print(\"Error creating DataFrame:\", e)\n",
        "else:\n",
        "    print(\"No tweets collected. DataFrame not created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1f032-2T_vn",
        "outputId": "2598493e-4054-470b-b9dd-5720d914fd56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No tweets collected. DataFrame not created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 'df' in locals():\n",
        "    try:\n",
        "        df.to_csv(\"tweets_data.csv\", index=False)\n",
        "        print(\"Tweets data saved to tweets_data.csv\")\n",
        "    except Exception as e:\n",
        "        print(\"Error saving DataFrame to CSV:\", e)\n",
        "else:\n",
        "    print(\"DataFrame not created; no data to save.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo2B1DecUxPW",
        "outputId": "eff41f50-f687-4c3f-9794-be91ce0ae45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweets data saved to tweets_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write your response here.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sZOhks1dXWEe"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}